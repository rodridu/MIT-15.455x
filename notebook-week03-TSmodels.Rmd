---
title: "Time Series Data and Models"
subtitle: "Model Identification and Estimation"
author: "Paul F. Mende"
date: "Summer 2021"
output: 
  html_notebook:
  df_print: paged
  toc: yes
---

# The right model?

Making inferences from real-world data and building effective models is a challenging process.  The data rarely fits exactly, and models may stop working.  So it always requires judgment, not just stats and number crunching, in applying modeling and forecasting techniques.   For that reason, Monte Carlo simulations provide an excellent testing laboratory for identifcation techniques.  We can be sure that a "right answer" exists, then see which analytics identify it and how much uncertainty remains in a best-case scenario.

- Given a model, estimate its parameters
- Given a class of models, determine the best


# Order determination:  AR(2) Example


```{r}
c_0   <-  0.001 
c_1   <- -0.1
c_2   <-  0.4
sigma <- 1
Nt    <-  1000; 
r     <-  matrix(0,Nt,1)
z     <- matrix(rnorm(Nt), ncol=1)

for (t in 3:Nt)  {
  r[t] <- c_0 + c_1*r[t-1] + c_2*r[t-2] + sigma*z[t]
}

plot(cumsum(r),type="l",main="AR(2) Sample Path",xlab="Time",ylab="r");grid()

acf(r, main="AR(2) Sample Autocorrelation Function")
pacf(r, main="AR(2) Sample Partial Autocorrelation Function")

```
# Model estimation: AR(2) example

AR models can be viewed and estimated as multivariate linear regression models, where the right-hand size "independent" variables are just the lagged observation series.  For the process to be consistent, the regression residuals must be (approximately) an independent, uncorrelated white noise process.

In R, there are functions that do the same thing.  The most general case, for ARIMA models, includes AR(p) as a special case of ARIMA(p,0,0)

```{r}
# Method (1) Numerical estimation using ordinary least squares
y   <- r[3:Nt]
x1  <- r[2:(Nt-1)]
x2  <- r[1:(Nt-2)]

AR2model <- lm(y ~ x1 + x2)
summary(AR2model)

# Method (2): Using the arima function
arima(r, order=c(2,0,0))
```

What if we get the order incorrect?  If the data is not too noisy, the redundant terms will have coefficients close to zero.

```{r}
arima(r, order=c(5,0,0))
```



# Order determination:  MA(2) Example


```{r}
mu  <-  0.0
sigma <-  1.0; 
phi_1 <- -0.1; 
phi_2 <-  0.4; 
Nt  <-  1000; 
r   <-  matrix(0,Nt,1)
z <- matrix(rnorm(Nt), ncol=1)

# Generate return series recursively.  Assign z=0 before initial observation to bootstrap.
r[1] <- mu + sigma*z[1]
r[2] <- mu + sigma*z[2] + phi_1*z[1]
for (t in 3:Nt)  {
  r[t] <- mu + sigma*z[t] + phi_1*z[t-1] + phi_2*z[t-2]
}

plot(cumsum(r),type="l",main="MA(2) Sample Path",xlab="Time",ylab="r");grid()

acf(r, main="MA(2) Sample Autocorrelation Function")
pacf(r, main="MA(2) Sample Partial Autocorrelation Function")

```
# Model estimation:  MA(2)

```{r}
arima(r,order = c(0,0,2))
```

# Real data is much harder

```{r Fetch some test data from Yahoo! Finance}

###install.packages("tidyquant")
library(tidyquant)

# Define query parameters
ticker <- "TR"
date_first <- "1987-12-31"
date_last  <- "2017-12-31"

#Get the data
TR <- tq_get(ticker, from=date_first, to=date_last)

```


Here is what the price looks like over time

```{r price charts}
plot(TR$date,TR$adjusted,type="l",xlab="Time",ylab="Price",main="TR Adjusted Price 1988-2017");grid()
```


Now we extract the time series of prices and compute the series of 1-day returns, 
$$r_t = \log(P_t/P_{t-1}) = \log(P_t/P_0) - \log(P_{t-1}/P_0)$$ .

```{r daily returns}

# Compute the returns.  Remember that the first return is recorded on the second trade date. That's why we retrieve prices before the first trade date of interest.

P    <- TR$adjusted
r    <- diff(log(P))
N    <- length(r)

# The returns can also be stored as a new column in TR.  

TR$r <- c(NA, diff(log(TR$adjusted)))

# Trim off the first row, which has return NA
TR   <- TR[-1,]



plot(TR$date,TR$r,type="l",xlab="Time",ylab="Price",main="TR Daily Returns 1988-2017");grid()
```

The daily return series is noisy, and the mean value is barely visible.  

Is the series stationary?  If it were, then the mean and volatility would be identical in every subsample, within sampling errors.  However the scale of the TR noise fluctuations is not constant over time.  This is the phenomenon of time-varying volatility.  Compare the graph above with the simulation below, in which simulated returns have the same average volatility and zero mean.

```{r white noise}
plot(TR$date,rnorm(nrow(TR))*sd(TR$r),type="l",ylim=c(-0.18,0.18),xlab="Time",ylab="Price",main="White Noise Process with TR Volatility");grid()
```


